"""
Dogukan Bayraktar
Snakemake version 7.3.1
Workflow for running and comparatively analyzing results from the three
RNA-seq pipelines TALON, FLAIR, and OXFORD
"""
# python strings as paths do not work with pathlib objects
from pathlib import Path

# All parameters must be set by the user in the configuration file

# Working directory
workdir: Path.joinpath(Path(config["run_name"]))
# Location of snakefile relative to workdir
SNAKEFILE = Path(workflow.snakefile)

# Path where you cloned the repository to
SOURCEDIR = Path(config["sourcedir"])
# output goes here
OUTDIR = Path.joinpath(Path(config["run_name"]), Path(config["results"]))
# Location of reference genome in fasta format
reference_genome = Path(config["genome_fasta"])
# Location of the annotation file
existing_annotation = Path(config["existing_annotation"])
# Directory containing only the fastq files
READSDIR = Path(config["readsdir"])
# List of fastq file names
READS = [filepath for filepath in Path(READSDIR).glob('**/*')]

# Sample names must be unique
if len(READS) != len(set(READS)):
    raise RuntimeError("\n*** Samplenames are NOT unique. Cannot continue! exiting. ***\n")

report: "report/workflow.rst"

rule all:
    input:
        # TALON
        # talon_abundance = OUTDIR / "02_TALON" / 'filtered_talon_abundance_filtered.tsv',
        # talon_GTF = OUTDIR / "02_TALON" / 'filtered_talon.gtf',
        #
        # FLAIR
        # flair_abundance = OUTDIR / "02_FLAIR" / "quantify" / "flair_counts_matrix.tsv",
        #
        # OXFORD
        # ox_transcripts = OUTDIR / "02_OXFORD" / "abundance" / "transcript_count_matrix.csv",
        #
        # ANALYSES
        # all_diagram = OUTDIR / "QC_notebook_graphs" / "all_overlap_diagram.png",
        # known_diagram = OUTDIR / "QC_notebook_graphs" / "known_overlap_diagram.png",
        # novel_diagram = OUTDIR / "QC_notebook_graphs" / "novel_overlap_diagram.png",
        # all_upset= OUTDIR / "QC_notebook_graphs" / "all_upset.png",
        # known_upset= OUTDIR / "QC_notebook_graphs" / "known_overlap_upset.png",
        # novel_upset= OUTDIR / "QC_notebook_graphs" / "novel_overlap_upset.png",
        # classes = OUTDIR / "QC_notebook_graphs" / "all_gffcompare_classes.png",
        # align = OUTDIR / "QC_notebook_graphs" / "read_alignments.png",
        # mismatch = OUTDIR / "QC_notebook_graphs" / "read_mismatches.png",
        # readlen = OUTDIR / "QC_notebook_graphs" / "read_lengths.png",
        # total = OUTDIR / "QC_notebook_graphs" / "total_count_per_sample.png",
        # per_feature = OUTDIR / "QC_notebook_graphs" / "counts_per_feature.png",
        # comp = OUTDIR / "QC_notebook_graphs" / "counts_per_pipeline.png",
        # oxford_flair = OUTDIR / "QC_exon_comparison" / "oxford_flair.png",
        # oxford_talon = OUTDIR / "QC_exon_comparison" / "oxford_talon.png",
        # flair_talon = OUTDIR / "QC_exon_comparison" / "flair_talon.png",
        # oxford_flair_summ = OUTDIR / "QC_exon_comparison" / "oxford_flair_summ.txt",
        # oxford_talon_summ = OUTDIR / "QC_exon_comparison" / "oxford_talon_summ.txt",
        # flair_talon_summ = OUTDIR / "QC_exon_comparison" / "flair_talon_summ.txt",
        # tracking_three = OUTDIR / "05_matched_transcripts" / "three_match.gtf",
        # tracking_two = OUTDIR / "05_matched_transcripts" / "two_match.gtf",
        # tracking_one = OUTDIR / "05_matched_transcripts" / "no_match.gtf",
        # classification = expand(OUTDIR / "QC_sqanti3" / "{gtf}" / "{gtf}_classification.txt", gtf=["02_TALON/filtered_talon", "02_FLAIR/collapse/flair.collapse.isoforms", "02_OXFORD/stringtie_output/oxford_merged"])
        plots = expand(OUTDIR / "QC_nanoplot" / "{sample}" / "{sample}_NanoStats.txt", sample=[".".join(read.name.split('.')[:-1]) for read in READS])

rule nanoplot:
    """
    Fastq Nanopore statistics
    """
    input:
        fq = READSDIR / "{sample}.fastq"
    params:
        prefix = lambda wildcards: wildcards.sample + '_',
        outdir = lambda wildcards: OUTDIR / "QC_nanoplot" / wildcards.sample
    output:
        plot =  OUTDIR / "QC_nanoplot" / "{sample}" / "{sample}_NanoStats.txt"
    threads: 10
    singularity: "docker://quay.io/biocontainers/nanoplot:1.40.0--pyhdfd78af_0"
    shell:
        '''
        NanoPlot \
        -t {threads} \
        --fastq {input.fq} \
        --plots dot kde \
        -p {params.prefix} \
        -o {params.outdir}
        '''

rule minimap2_align:
    """
    Align fastq files to reference genome.
    """
    input:
        genome = reference_genome,
        fq = READSDIR / "{sample}.fastq"
    params:
        outdir = OUTDIR,
        opts = config["minimap2_opts"]
    output:
        sam_files = OUTDIR / "01_alignments" / "{sample}.sam"
    threads: 10
    singularity:
        'docker://quay.io/biocontainers/minimap2:2.17--h5bf99c6_4'
    shell:
        '''
        minimap2 \
            -t {threads} \
            -ax splice \
            {params.opts} \
            --MD \
            {input.genome} \
            {input.fq} > {output.sam_files}
        '''

rule sam_to_bam:
    """"
    Converts SAM to BAM. 
    """
    input:
        sam = rules.minimap2_align.output
    params:
        outdir = lambda wildcards: OUTDIR / "01_alignments" / "bam" / wildcards.sample
    output:
        bam = OUTDIR / "01_alignments" / "bam" / "{sample}_sorted.bam"
    threads: 10
    singularity:
        "docker://quay.io/biocontainers/samtools:1.14--hb421002_0"
    shell:
        '''
        samtools view -Sb {input.sam} | samtools sort -@ {threads} -o {output.bam}
        samtools index {output.bam}
        '''


##################
#      TALON     #
##################

rule get_SJs_from_gtf:
    """
    Extracts splice junctions from annotation file for use with 
    TranscriptClean.
    """
    input:
        annotation = existing_annotation,
        genome = reference_genome
    params:
        outdir = OUTDIR
    output:
        splicejns = OUTDIR / "02_TALON" / "cleaned_alignments" / "spliceJns.txt"
    threads: 1
    singularity:
        "docker://biocontainers/transcriptclean:v2.0.2_cv1"
    shell:
        '''
        get_SJs_from_gtf \
            --f {input.annotation} \
            --g {input.genome} \
            --o {output.splicejns}
        '''

rule transcriptclean:
    """
    Corrects artefactual noncanonical splice junctions.
    """
    input:
        sam_files = rules.minimap2_align.output.sam_files,
        genome = reference_genome,
        splicejns = rules.get_SJs_from_gtf.output.splicejns
    params:
        outdir = lambda wildcards: OUTDIR / "02_TALON" / "cleaned_alignments" / wildcards.sample / wildcards.sample
    output:
        clean_sam = OUTDIR / "02_TALON" / "cleaned_alignments" / "{sample}" / "{sample}_clean.sam"
    threads: 10
    singularity:
        "docker://biocontainers/transcriptclean:v2.0.2_cv1"
    shell:
        '''
        TranscriptClean \
            --sam {input.sam_files} \
            --genome {input.genome} \
            -t {threads} \
            --spliceJns {input.splicejns} \
            --outprefix {params.outdir}
        '''

rule talon_label_reads:
    """
    Flag possible internal priming in reads.
    """
    input:
        clean_sam = rules.transcriptclean.output.clean_sam,
        genome = reference_genome
    params:
        outdir = lambda wildcards: OUTDIR / "02_TALON" / "labeled" / wildcards.sample,
    output:
        labeled_sam = OUTDIR / "02_TALON" / "labeled" / "{sample}_labeled.sam",
    threads: 10
    singularity:
        "docker://biocontainers/talon:v5.0_cv1"
    shell:
        '''
        talon_label_reads \
            --f {input.clean_sam}\
            --g {input.genome} \
            --t {threads} \
            --ar 20 \
            --deleteTmp \
            --o {params.outdir}
        '''

rule create_talon_config:
    """
    Create configuration file containing labeled reads locations.
    """
    input:
        labels = expand(OUTDIR / "02_TALON" / "labeled" / "{sample}_labeled.sam",sample=[".".join(read.name.split('.')[:-1]) for read in READS]),
    params:
        datasetnames= [".".join(read.name.split('.')[:-1]) for read in READS]
    output:
        config = OUTDIR / "02_TALON" / "config.csv",
    threads: 1
    run:
        for label, name in zip(input.labels, params.datasetnames):
            with open(output.config, 'a+') as config:
                config.write("%s,%s,ONT,%s\n" % (name, name, label))

rule talon_initialize_annotate_database:
    """
    Initialize TALON database from annotation file. Annotate transcripts
    by comparing them to the database and update database accordingly
    afterwards.
    """
    input:
        annotation = existing_annotation,
        config = rules.create_talon_config.output.config
    params:
        outdir = OUTDIR / "02_TALON" / "talon",
        dbloc = OUTDIR / "02_TALON" / "talon.db",
        annotation_name = existing_annotation.name.split('.')[0],
        genome_name = reference_genome.name.split('.')[0],
        end5 = config["talon_5p"],
        end3 = config["talon_3p"],
    output:
        database = OUTDIR / "02_TALON" / "talon.db"
    threads: 10
    singularity:
        "docker://biocontainers/talon:v5.0_cv1"
    shell:
        '''
        
        talon_initialize_database \
            --f {input.annotation} \
            --a {params.annotation_name} \
            --g {params.genome_name} \
            --5p {params.end5} \
            --3p {params.end3} \
            --o {params.outdir}
        echo database init done
        
        talon \
            --f {input.config} \
            --db {params.dbloc} \
            --build {params.genome_name} \
            --o {params.outdir} \
            -t {threads}
        echo database annotation done
        '''

rule talon_filter_transcripts:
    """
    Create whitelist of transcripts that passed filter.
    """
    input:
        db = rules.talon_initialize_annotate_database.output.database
    params:
        outdir = OUTDIR,
        datasetnames = ",".join([".".join(read.name.split('.')[:-1]) for read in READS]),
        mincount = config["talon_mincounts"],
        mindatasets = config["talon_mindatasets"],
        annotation_name = existing_annotation.name.split('.')[0],
    output:
        filtered_transcripts = OUTDIR / "02_TALON" / "filtered_transcripts.csv"
    threads: 1
    singularity:
        "docker://biocontainers/talon:v5.0_cv1"
    shell:
        '''
        talon_filter_transcripts \
            --db {input.db} \
            --datasets {params.datasetnames} \
            -a {params.annotation_name} \
            --maxFracA 0.5 \
            --minCount {params.mincount} \
            --minDatasets {params.mindatasets} \
            --o {output.filtered_transcripts}
        '''

rule talon_abundance:
    """
    Calculate abundance for transcripts in whitelist.
    """
    input:
        db = rules.talon_initialize_annotate_database.output.database,
        filter = rules.talon_filter_transcripts.output.filtered_transcripts
    params:
        outdir = OUTDIR / "02_TALON" / 'filtered',
        annotation_name = existing_annotation.name.split('.')[0],
        genome_name= reference_genome.name.split('.')[0],
    output:
        abundance = OUTDIR / "02_TALON" / 'filtered_talon_abundance_filtered.tsv'
    singularity:
        "docker://biocontainers/talon:v5.0_cv1"
    shell:
        '''
        talon_abundance \
            --db {input.db} \
            --whitelist {input.filter} \
            -a {params.annotation_name} \
            --build {params.genome_name} \
            --o {params.outdir}
        '''

rule talon_create_GTF:
    """
    Extract transcripts from whitelist as GTF.
    """
    input:
        db = rules.talon_initialize_annotate_database.output.database,
        filter= rules.talon_filter_transcripts.output.filtered_transcripts
    params:
        outdir = OUTDIR / "02_TALON" / 'filtered',
        annotation_name = existing_annotation.name.split('.')[0],
        genome_name= reference_genome.name.split('.')[0],
    output:
        GTF = OUTDIR / "02_TALON" / 'filtered_talon.gtf'
    singularity:
        "docker://biocontainers/talon:v5.0_cv1"
    shell:
        '''
        talon_create_GTF \
            --db {input.db} \
            --whitelist {input.filter} \
            -a {params.annotation_name} \
            --build {params.genome_name} \
            --o {params.outdir}
        '''


##################
#      FLAIR     #
##################

rule flair_bam_to_bed12:
    """
    Convert BAM files to BED12 files. Also "smooths" gaps in the
    alignments.
    """
    input:
        bam = rules.sam_to_bam.output.bam
    params:
        outdir = lambda wildcards: OUTDIR / "02_FLAIR" / "bed12" / wildcards.sample,
        scriptpath = SNAKEFILE.parent / "scripts" / "flair_scripts" / "bam2Bed12.py"
    output:
        bed12 = OUTDIR / "02_FLAIR" / "bed12" / "{sample}.bed"
    threads: 1
    conda:
        "envs/bam2bed12.yaml"
    shell:
        '''
        {params.scriptpath} --input_bam {input.bam} > {output.bed12}
        '''

rule flair_correct:
    """
    Corrects misaligned splice sites using genome annotations
    and/or short-read splice junctions.
    """
    input:
        genome = reference_genome,
        annotation= existing_annotation,
        # bed = rules.flair_align.output.bed
        bed = rules.flair_bam_to_bed12.output.bed12
    params:
        outdir = lambda wildcards: OUTDIR / "02_FLAIR" / "corrected" / wildcards.sample / wildcards.sample,
        window = config["flair_correct_window"]
    output:
        bed_corrected = OUTDIR / "02_FLAIR" / "corrected" / "{sample}" / "{sample}_all_corrected.bed"
    threads: 10
    singularity:
        'docker://quay.io/biocontainers/flair:1.5--hdfd78af_4'
    shell:
        '''
        flair.py correct \
            --genome {input.genome} \
            --query {input.bed} \
            --gtf {input.annotation} \
            --nvrna \
            --threads {threads} \
            --window {params.window} \
            --output {params.outdir}
        '''

rule flair_concatenate:
    """
    Combines BED12 output into one file.
    """
    input:
        bed_corrected = expand(OUTDIR / "02_FLAIR" / "corrected" / "{sample}" / "{sample}_all_corrected.bed", sample=[".".join(read.name.split('.')[:-1]) for read in READS])
    output:
        bed_concatenated = OUTDIR / "02_FLAIR" / "concatenated_all_corrected.bed"
    threads: 10
    singularity:
        'docker://quay.io/biocontainers/flair:1.5--hdfd78af_4'
    shell:
        '''
        cat {input.bed_corrected} >> {output.bed_concatenated}
        '''

rule flair_collapse:
    """
    Defines high-confidence isoforms from corrected reads.
    """
    input:
        bed_concatenated = rules.flair_concatenate.output.bed_concatenated,
        genome = reference_genome,
        annotation = existing_annotation,
    params:
        reads = READS,
        temp_dir = OUTDIR / "02_FLAIR" / "collapse" / "collapse_logs",
        outdir = OUTDIR / "02_FLAIR" / "collapse" / "flair.collapse",
        window = config["flair_collapse_window"],
        quality = config["flair_collapse_quality"],
        support = config["flair_collapse_support"],
        max_ends = config["flair_collapse_max_ends"],
        opts = config["flair_collapse_optional"]
    output:
        fa = OUTDIR / "02_FLAIR" / "collapse" / "flair.collapse.isoforms.fa",
        gtf = OUTDIR / "02_FLAIR" / "collapse" / "flair.collapse.isoforms.gtf"
    threads: 10
    singularity:
        'docker://quay.io/biocontainers/flair:1.5--hdfd78af_4'
    shell:
        '''
        flair.py collapse \
            --genome {input.genome} \
            --gtf {input.annotation} \
            --reads {params.reads} \
            --query {input.bed_concatenated} \
            --temp_dir {params.temp_dir} \
            --generate_map \
            --threads {threads} \
            --window {params.window} \
            --quality {params.quality} \
            --support {params.support} \
            --max_ends {params.max_ends} \
            {params.opts} \
            --output {params.outdir}
        '''

rule flair_config:
    """
    Creates read manifest.
    """
    input:
        reads = READS
    params:
        datasetnames= [".".join(read.name.split('.')[:-1]) for read in READS]
    output:
        config = OUTDIR / "02_FLAIR" / "manifest.tsv"
    threads: 1
    run:
        for read, name in zip(input.reads, params.datasetnames):
            with open(output.config, 'a+') as config:
                config.write("%s\tcondition\tbatch\t%s\n" % (name, read))

rule flair_quantify:
    """
    Quantify FLAIR isoform usage across samples using minimap2.
    """
    input:
        manifest = rules.flair_config.output.config,
        coll_fasta = rules.flair_collapse.output.fa
    params:
        quality = config["flair_abundance_quality"]
    output:
        abundance = OUTDIR / "02_FLAIR" / "quantify" / "flair_counts_matrix.tsv"
    threads: 10
    singularity:
        'docker://quay.io/biocontainers/flair:1.5--hdfd78af_4'
    shell:
        '''
        flair.py quantify \
            --reads_manifest {input.manifest} \
            --isoforms {input.coll_fasta} \
            --threads {threads} \
            --tpm \
            --quality {params.quality} \
            --output {output.abundance}
        '''

# generate statistics

rule samtool_stats:
    """
    Create alignment statistics with samtools.
    """
    input:
        bam = rules.sam_to_bam.output.bam,
        reference = reference_genome,
    params:
        outdir = lambda wildcards: OUTDIR / "alignments" / "stats" / wildcards.sample
    output:
        stats = OUTDIR  / "QC_statistics" / "{sample}.stats"
    threads: 10
    singularity:
        "docker://quay.io/biocontainers/samtools:1.14--hb421002_0"
    shell:
        '''
        samtools stats \
            --reference {input.reference} \
            --threads {threads} \
            {input.bam} | grep ^SN | cut -f 2- > {output.stats}
        '''


##################
#      OXFORD    #
##################

ContextFilter = """AlnContext: { Ref: "%s", LeftShift: -%d, RightShift: %d, RegexEnd: "[Aa]{%d,}", Stranded: True, Invert: True, Tsv: "alignments/internal_priming_fail.tsv"} """ % (reference_genome,
config["oxford_poly_context"], config["oxford_poly_context"], config["oxford_max_poly_run"])

rule oxford_filter:
    """
    Filters bam files and checks for internal priming.
    """
    input:
        genome = reference_genome,
        bam = rules.sam_to_bam.output.bam
    params:
        min_mq = config["oxford_minimum_mapping_quality"],
        flt = lambda x: ContextFilter,
    output:
        bam_filtered = OUTDIR / "02_OXFORD" / "filtered"/ "{sample}.bam"
    threads: 10
    conda:
        "envs/oxford_filter.yaml"
    shell:
        '''
        samtools view -q {params.min_mq} -F 2304 -b {input.bam} \
        | seqkit bam -j {threads} -x -T '{params.flt}' \
        | samtools sort -@ {threads} -o {output.bam_filtered}
        samtools index {output.bam_filtered}
        '''

rule oxford_run_stringtie:
    """
    Run stringtie for annotation per sample.
    """
    input:
        bam_filtered = rules.oxford_filter.output.bam_filtered,
        annotation = existing_annotation,
    params:
        opts = config["oxford_stringtie_opts"],
    output:
        gff = OUTDIR / "02_OXFORD" / "stringtie_output" / "{sample}.gff",
    singularity:
        "docker://quay.io/biocontainers/stringtie:2.2.1--hecb563c_2"
    threads: 10
    shell:
        '''
        stringtie --rf -G {input.annotation} -L -p {threads} {params.opts} -o {output.gff} {input.bam_filtered}
        '''

rule oxford_merge_stringtie:
    """
    Merge sample annotations to single file.
    """
    input:
        gff_files = expand(OUTDIR / "02_OXFORD" / "stringtie_output" / "{sample}.gff", sample=[".".join(read.name.split('.')[:-1]) for read in READS]),
        annotation = existing_annotation
    params:
        opts = config["oxford_merge_opts"],
    output:
        merged_gff = OUTDIR / "02_OXFORD" / "stringtie_output" / "oxford_merged.gtf"
    singularity:
        "docker://quay.io/biocontainers/stringtie:2.2.1--hecb563c_2"
    threads: 10
    shell:
        '''
        stringtie \
            --merge {input.gff_files} \
            -G {input.annotation} \
            {params.opts} \
            -o {output.merged_gff}
        '''

rule oxford_abundance:
    """
    Run stringtie in expression estimation mode for all samples.
    """
    input:
        bam = rules.oxford_filter.output.bam_filtered,
        merged_gtf = rules.oxford_merge_stringtie.output.merged_gff
    params:
        opts = config["oxford_count_opts"],
    output:
        count_gtf = OUTDIR / "02_OXFORD" / "abundance" / "{sample}.gtf"
    singularity:
        "docker://quay.io/biocontainers/stringtie:2.2.1--hecb563c_2"
    threads: 10
    shell:
        '''
        stringtie \
            -G {input.merged_gtf} \
            -e \
            {params.opts} \
            -o {output.count_gtf} \
            {input.bam}
        '''

rule oxford_config:
    """
    Create configuration file containing location of abundance
    GTF files.
    """
    input:
        count_gtf = expand(OUTDIR / "02_OXFORD" / "abundance" / "{sample}.gtf", sample=[".".join(read.name.split('.')[:-1]) for read in READS]),
    params:
        datasetnames= [".".join(read.name.split('.')[:-1]) for read in READS]
    output:
        config = OUTDIR / "02_OXFORD" / "abundance" / "oxford_config.tab"
    threads: 1
    run:
        for read, name in zip(input.count_gtf, params.datasetnames):
            with open(output.config, 'a+') as config:
                config.write("%s\t%s\n" % (name, read))

rule oxford_calc_abundance:
    """
    Calculate abundance.
    """
    input:
        config_file = rules.oxford_config.output.config,
        stats = expand(OUTDIR / "QC_statistics" / "{sample}.stats", sample=[".".join(read.name.split('.')[:-1]) for read in READS])
    output:
        transcripts = OUTDIR / "02_OXFORD" / "abundance" / "transcript_count_matrix.csv",
        genes = OUTDIR / "02_OXFORD" / "abundance" / "gene_count_matrix.csv",
    threads: 10
    singularity:
        "docker://quay.io/biocontainers/stringtie:2.2.1--hecb563c_2"
    shell:
        '''
        declare -i calc=0;
        for line in {input.stats}
        do
           numb=$(awk 'NR==26 {{print $3}}' < ${{line}});
           calc+=$((${{numb}}));
        done;
        words=$(echo {input.stats} | wc -w);
        avg_len=$((${{calc}} / ${{words}}));

        prepDE.py \
            -i {input.config_file} \
            -l ${{avg_len}} \
            -g {output.genes} \
            -t {output.transcripts}
        '''

##################
#    analyses    #
##################

rule sqanti3:
    input:
        gtf_input = OUTDIR / "{gtf}.gtf",
        annotation = existing_annotation,
        genome = reference_genome,
    params:
        outdir = lambda wildcards: OUTDIR / "QC_sqanti3" / wildcards.gtf
    output:
        classification = OUTDIR / "QC_sqanti3" / "{gtf}" / "{gtf}_classification.txt"
        # classification = str(OUTDIR) + "/" + "QC_sqanti3" + "/" + "{gtf}"  + "/" + "{gtf}" + "_classification.txt"
    threads: 10
    singularity:
        "/exports/sascstudent/dbayraktar/sqanti3_4.2_docker.sif"
    shell:
        '''
        sqanti3_qc.py \
            {input.gtf_input} \
            {input.annotation} \
            {input.genome} \
            --force_id_ignore \
            --output {params.outdir} \
            --cpus {threads} \
            --report both
        '''

rule subread_featurecounts:
    """
    Calculate counts from the annotation file with Subread.
    """
    input:
        bams = expand(OUTDIR / "01_alignments" / "bam" / "{sample}_sorted.bam",sample=[".".join(read.name.split('.')[:-1]) for read in READS]),
        annotation = existing_annotation,
    output:
        counts = OUTDIR / "QC_statistics" / "annotation_count.txt"
    threads: 10
    singularity:
        "docker://quay.io/biocontainers/subread:2.0.1--h7132678_2"
    shell:
        '''
        featureCounts \
            {input.bams} \
            -T {threads} \
            -a {input.annotation} \
            -L \
            -M \
            -S 1 \
            -o {output.counts}
        '''

rule combine:
    """
    Adds counts from the abundance file to the transcripts in the GTF
    file.
    """
    input:
        oxford_count = rules.oxford_calc_abundance.output.transcripts,
        oxford_transcript = rules.oxford_merge_stringtie.output.merged_gff,
        talon_count = rules.talon_abundance.output.abundance,
        talon_transcripts = rules.talon_create_GTF.output.GTF,
        flair_count = rules.flair_quantify.output.abundance,
        flair_transcripts = rules.flair_collapse.output.gtf,
    params:
        scriptpath = SNAKEFILE.parent / "scripts" / "combine.py"
    output:
        oxford_combined = OUTDIR / "03_combined" / "oxford_counts.gtf",
        flair_combined = OUTDIR / "03_combined" / "flair_counts.gtf",
        talon_combined = OUTDIR / "03_combined" / "talon_counts.gtf",
        missing_transcripts = OUTDIR / "03_combined" / "missing_transcripts.log",
    threads: 1
    script:
        '''scripts/combine.py'''

rule gffcompare:
    """
    Run GFFcompare for matching transcripts from the pipelines.
    """
    input:
        oxford = rules.combine.output.oxford_combined,
        flair = rules.combine.output.flair_combined,
        talon = rules.combine.output.talon_combined,
        annotation = existing_annotation
    params:
        outdir = OUTDIR / "04_gffcompare" / "gffcmp"
    output:
        tracking = OUTDIR / "04_gffcompare" / "gffcmp.tracking"
    threads: 1
    singularity:
        "docker://quay.io/biocontainers/gffcompare:0.11.2--h9f5acd7_3"
    shell:
        '''
        gffcompare \
            -r {input.annotation} \
            -e 100 \
            -d 100 \
            --no-merge \
            -o {params.outdir} \
            {input.oxford} {input.flair} {input.talon}
        '''

rule extract_matches:
    """
    Extract transcript matches into GTF files.
    """
    input:
        oxford = rules.combine.output.oxford_combined,
        flair = rules.combine.output.flair_combined,
        talon = rules.combine.output.talon_combined,
        tracking = rules.gffcompare.output.tracking,
    params:
        scriptpath = SNAKEFILE.parent / "scripts" / "extract_overlap.py"
    output:
        tracking_three = OUTDIR / "05_matched_transcripts" / "three_match.gtf",
        tracking_two = OUTDIR / "05_matched_transcripts" / "two_match.gtf",
        tracking_one = OUTDIR / "05_matched_transcripts" / "no_match.gtf",
    script:
        '''scripts/extract_overlap.py'''

rule venn_diagrams:
    """
    Create venn diagrams.
    """
    input:
        tracking = rules.gffcompare.output.tracking
    output:
        all_diagram = report(OUTDIR / "QC_notebook_graphs" / "all_overlap_diagram.png", caption="report/transcript_overlap/all_diagram.rst", category="Transcript overlap graphs"),
        known_diagram = report(OUTDIR / "QC_notebook_graphs" / "known_overlap_diagram.png", caption="report/transcript_overlap/known_diagram.rst", category="Transcript overlap graphs"),
        novel_diagram = report(OUTDIR / "QC_notebook_graphs" / "novel_overlap_diagram.png", caption="report/transcript_overlap/novel_diagram.rst", category="Transcript overlap graphs"),
        all_upset = report(OUTDIR / "QC_notebook_graphs" / "all_upset.png", caption="report/transcript_overlap/all_upset.rst", category="Transcript overlap graphs"),
        known_upset = report(OUTDIR / "QC_notebook_graphs" / "known_overlap_upset.png", caption="report/transcript_overlap/known_upset.rst", category="Transcript overlap graphs"),
        novel_upset = report(OUTDIR / "QC_notebook_graphs" / "novel_overlap_upset.png", caption="report/transcript_overlap/novel_upset.rst", category="Transcript overlap graphs"),
        classes = report(OUTDIR / "QC_notebook_graphs" / "all_gffcompare_classes.png", caption="report/transcript_overlap/classes.rst", category="Transcript overlap graphs"),
    threads: 1
    conda:
        "envs/notebooks.yaml"
    log:
        notebook = OUTDIR / "QC_notebook_graphs" / "processed_venndiagrams.ipynb"
    notebook:
        "notebooks/venndiagrams.py.ipynb"

rule alignment_stats:
    """
    Create graphs showing alignment statistics.
    """
    input:
        stats = expand(OUTDIR / "QC_statistics" / "{sample}.stats" , sample=[".".join(read.name.split('.')[:-1]) for read in READS]),
    params:
        datasetnames = [".".join(read.name.split('.')[:-1]) for read in READS]
    output:
        align = report(OUTDIR / "QC_notebook_graphs" / "read_alignments.png", caption="report/alignment/align.rst", category="Alignment statistics"),
        mismatch = report(OUTDIR / "QC_notebook_graphs" / "read_mismatches.png", caption="report/alignment/mismatch.rst", category="Alignment statistics"),
        readlen = report(OUTDIR / "QC_notebook_graphs" / "read_lengths.png", caption="report/alignment/readlen.rst", category="Alignment statistics"),
    threads: 1
    conda:
        "envs/notebooks.yaml"
    log:
        notebook = OUTDIR / "QC_notebook_graphs" / "processed_align_stats.ipynb"
    notebook:
        "notebooks/align_stats.py.ipynb"

rule count_stats:
    """
    Create graphs showing abundance statistics
    """
    input:
        subread_count = rules.subread_featurecounts.output.counts,
        flair_count = rules.flair_quantify.output.abundance,
        talon_count = rules.talon_abundance.output.abundance,
        oxford_count = rules.oxford_calc_abundance.output.transcripts,
    params:
        datasetnames = [".".join(read.name.split('.')[:-1]) for read in READS]
    output:
        total = report(OUTDIR / "QC_notebook_graphs" / "total_count_per_sample.png", caption="report/abundance/total.rst", category="Abundance graphs"),
        per_feature = report(OUTDIR / "QC_notebook_graphs" / "counts_per_feature.png", caption="report/abundance/per_feature.rst", category="Abundance graphs"),
        comp = report(OUTDIR / "QC_notebook_graphs" / "counts_per_pipeline.png", caption="report/abundance/comp.rst", category="Abundance graphs"),
    threads: 1
    conda:
        "envs/notebooks.yaml"
    log:
        notebook = OUTDIR / "QC_notebook_graphs" / "processed_count_stats.ipynb"
    notebook:
        "notebooks/count_stats.py.ipynb"

rule exon_comparison:
    """
    Compare start and end positions of exons from matched transcripts.
    Create boxplots and summary of differences.
    """
    input:
        tracking = rules.gffcompare.output.tracking,
        oxford = rules.combine.output.oxford_combined,
        flair = rules.combine.output.flair_combined,
        talon = rules.combine.output.talon_combined,
    output:
        oxford_flair = report(OUTDIR / "QC_exon_comparison" / "oxford_flair.png", caption="report/exon/exon.rst", category="GFFcompare exon matching boxplots"),
        oxford_talon = report(OUTDIR / "QC_exon_comparison" / "oxford_talon.png", caption="report/exon/exon.rst", category="GFFcompare exon matching boxplots"),
        flair_talon = report(OUTDIR / "QC_exon_comparison" / "flair_talon.png", caption="report/exon/exon.rst", category="GFFcompare exon matching boxplots"),
        oxford_flair_summ = report(OUTDIR / "QC_exon_comparison" / "oxford_flair_summ.txt", caption="report/exon/exon_sum.rst", category="GFFcompare exon matching boxplots"),
        oxford_talon_summ = report(OUTDIR / "QC_exon_comparison" / "oxford_talon_summ.txt", caption="report/exon/exon_sum.rst", category="GFFcompare exon matching boxplots"),
        flair_talon_summ = report(OUTDIR / "QC_exon_comparison" / "flair_talon_summ.txt", caption="report/exon/exon_sum.rst", category="GFFcompare exon matching boxplots"),
    threads: 1
    conda:
        "envs/notebooks.yaml"
    script: '''scripts/exon_comparison.py'''
