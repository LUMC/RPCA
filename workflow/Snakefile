
from pathlib import Path

# Read configuration
configfile: "config/config.yaml"

SOURCEDIR = Path(config["sourcedir"])
OUTDIR = Path(config["results"])
READSDIR = Path(config["readsdir"])
genome_fasta = Path(config["genome_fasta"])
existing_annotation = Path(config["existing_annotation"])


READS = [filepath for filepath in Path(READSDIR).glob('**/*')]

rule all:
    input:
        # abundance=OUTDIR / "TALON" / 'filtered_talon_abundance_filtered.tsv',
        # GTF = OUTDIR / "TALON" / 'filtered_talon.gtf',
        # bam = expand(OUTDIR / "alignments" / "BAM" / "{sample}_sorted.bam", sample=[ ".".join(read.name.split('.')[:-1]) for read in READS]),
        # bed12 = expand(OUTDIR / "FLAIR" / "BED12" / "{sample}.bed", sample=[ ".".join(read.name.split('.')[:-1]) for read in READS])
        # bed = expand(OUTDIR / "FLAIR" / "BED12" / "{sample}.bed", sample=[ ".".join(read.name.split('.')[:-1]) for read in READS])
        # bed_corrected = expand(OUTDIR / "FLAIR" / "corrected" / "{sample}" / "{sample}_all_corrected.bed", sample=[ ".".join(read.name.split('.')[:-1]) for read in READS])
        # bed_concatenated = OUTDIR / "FLAIR" / "concatenated_all_corrected.bed"
        # gtf = OUTDIR / "FLAIR" / "COLLAPSE" / "flair.collapse.isoforms.gtf"
        # config = OUTDIR / "FLAIR" / "manifest.tsv"
        # abundance = OUTDIR / "FLAIR" / "quantify" / "flair_counts_matrix.tsv"
        abundance = OUTDIR / "OXFORD" / "abundance" / "transcript_count_matrix.csv"


rule minimap2_align:
    '''
    Align reads to reference genome.
    '''
    input:
        genome = genome_fasta,
        fq = READSDIR / "{sample}.fastq"
    params:
        outdir = OUTDIR,
        opts = config["minimap2_opts"]
    output:
        sam_files = OUTDIR / "alignments" /"{sample}.sam"
    threads: 10
    singularity:
        'docker://quay.io/biocontainers/minimap2:2.17--h5bf99c6_4'
    shell:
        '''
        minimap2 \
            -t {threads} \
            -ax splice \
            {params.opts} \
            {input.genome} \
            {input.fq} > {output.sam_files}
        '''

rule build_minimap_index:
    input:
        genome = genome_fasta
    output:
        index = OUTDIR / "alignments" / "genome_index.mmi"
    params:
        opts = config["minimap_index_opts"]
    singularity:
        'docker://quay.io/biocontainers/minimap2:2.17--h5bf99c6_4'
    threads: 10
    shell:
        """
        minimap2 -t {threads} {params.opts} -I 1000G -d {output.index} {input.genome}
        """

rule sam_to_bam:
    '''
    Converts SAM to BAM. 
    '''
    input:
        sam = rules.minimap2_align.output
    params:
        outdir = lambda wildcards: OUTDIR / "alignments" / "BAM" / wildcards.sample
    output:
        bam = OUTDIR / "alignments" / "BAM" / "{sample}_sorted.bam"
    threads: 10
    singularity:
        "docker://quay.io/biocontainers/samtools:1.14--hb421002_0"
    shell:
        '''
        samtools view -Sb {input.sam} | samtools sort -@ {threads} -o {output.bam}
        samtools index {output.bam}
        '''

# TALON
rule get_SJs_from_gtf:
    '''
    Extracts splice junctions from annotation file for use with 
    TranscriptClean.
    '''
    input:
        annotation = existing_annotation,
        genome = genome_fasta
    params:
        outdir = OUTDIR
    output:
        splicejns = OUTDIR / "TALON" / "cleaned_alignments" / "spliceJns.txt"
    threads: 1
    singularity:
        "docker://biocontainers/transcriptclean:v2.0.2_cv1"
    shell:
        '''
        get_SJs_from_gtf \
            --f {input.annotation} \
            --g {input.genome} \
            --o {output.splicejns}
        '''

rule transcriptclean:
    '''
    Corrects artefactual noncanonical splice junctions.
    '''
    input:
        sam_files = rules.minimap2_align.output.sam_files,
        genome = genome_fasta,
        splicejns = rules.get_SJs_from_gtf.output.splicejns
    params:
        outdir = lambda wildcards: OUTDIR / "TALON" / "cleaned_alignments" / wildcards.sample / wildcards.sample
    output:
        clean_sam = OUTDIR / "TALON" /"cleaned_alignments" / "{sample}" / "{sample}_clean.sam"
    threads: 10
    singularity:
        "docker://biocontainers/transcriptclean:v2.0.2_cv1"
    shell:
        '''
        TranscriptClean \
            --sam {input.sam_files} \
            --genome {input.genome} \
            -t {threads} \
            --spliceJns {input.splicejns} \
            --outprefix {params.outdir}
        '''

rule talon_label_reads:
    '''
    Flag possible internal priming in reads.
    '''
    input:
        clean_sam = rules.transcriptclean.output.clean_sam,
        genome = genome_fasta
    params:
        outdir = lambda wildcards: OUTDIR / "TALON" / "labeled" / wildcards.sample
    output:
        labeled_sam = OUTDIR / "TALON" / "labeled" / "{sample}_labeled.sam",
    threads: 10
    singularity:
        "docker://biocontainers/talon:v5.0_cv1"
    shell:
        '''
        mkdir -p labeled
        talon_label_reads \
            --f {input.clean_sam}\
            --g {input.genome} \
            --t {threads} \
            --ar 20 \
            --o {params.outdir}
        '''

rule create_talon_config:
    '''
    Create configuration file containing labeled reads locations.
    '''
    input:
        labels = expand(OUTDIR / "TALON" / "labeled" / "{sample}_labeled.sam",sample=[".".join(read.name.split('.')[:-1]) for read in READS]),
    params:
        datasetnames= [".".join(read.name.split('.')[:-1]) for read in READS]
    output:
        config = OUTDIR / "TALON" / "config.csv",
    threads: 1
    run:
        for label, name in zip(input.labels, params.datasetnames):
            with open(output.config, 'a+') as config:
                config.write("%s,%s,ONT,%s\n" % (name, name, label))

rule talon_initialize_annotate_database:
    '''
    Initialize TALON database from annotation file.
    Annotate transcripts by comparing them to the database and update 
    database accordingly afterwards.
    '''
    input:
        annotation = existing_annotation,
        config = rules.create_talon_config.output.config
    params:
        outdir = OUTDIR / "TALON" / "talon",
        dbloc = OUTDIR / "TALON" / "talon.db",
        annotation_name = existing_annotation.name.split('.')[0],
        genome_name = genome_fasta.name.split('.')[0],
    output:
        database = OUTDIR / "TALON" / "talon.db"
    threads: 4
    singularity:
        "docker://biocontainers/talon:v5.0_cv1"
    shell:
        '''
        
        talon_initialize_database \
            --f {input.annotation} \
            --a {params.annotation_name} \
            --g {params.genome_name} \
            --5p 500 \
            --3p 300 \
            --o {params.outdir}
        echo "completed init"
        
        talon \
            --f {input.config} \
            --db {params.dbloc} \
            --build {params.genome_name} \
            --o {params.outdir} \
            -t {threads}
        echo "completed annot"
        '''

rule talon_filter_transcripts:
    '''
    Create whitelist of transcripts that passed filter.
    '''
    input:
        db = rules.talon_initialize_annotate_database.output.database
    params:
        outdir = OUTDIR,
        datasetnames= [".".join(read.name.split('.')[:-1]) for read in READS],
        mincount= config["mincounts"],
        mindatasets= config["mindatasets"],
        annotation_name = existing_annotation.name.split('.')[0],
    output:
        filtered_transcripts = OUTDIR / "TALON" / "filtered_transcripts.csv"
    threads: 1
    singularity:
        "docker://biocontainers/talon:v5.0_cv1"
    shell:
        '''
        talon_filter_transcripts \
            --db {input.db} \
            --datasets {params.datasetnames} \
            -a {params.annotation_name} \
            --maxFracA 0.5 \
            --minCount {params.mincount} \
            --minDatasets {params.mindatasets} \
            --o {output.filtered_transcripts}
        '''

rule talon_abundance:
    '''
    Calculate abundance for transcripts in whitelist.
    '''
    input:
        db = rules.talon_initialize_annotate_database.output.database,
        filter = rules.talon_filter_transcripts.output.filtered_transcripts
    params:
        outdir = OUTDIR / "TALON" / 'filtered',
        annotation_name = existing_annotation.name.split('.')[0],
    output:
        abundance = OUTDIR / "TALON" / 'filtered_talon_abundance_filtered.tsv'
    singularity:
        "docker://biocontainers/talon:v5.0_cv1"
    shell:
        '''
        talon_abundance \
            --db {input.db} \
            --whitelist {input.filter} \
            -a {params.annotation_name} \
            --build "SIRV" \
            --o {params.outdir}
        '''

rule talon_create_GTF:
    '''
    Extract transcripts from whitelist as GTF.
    '''
    input:
        db = rules.talon_initialize_annotate_database.output.database,
        filter= rules.talon_filter_transcripts.output.filtered_transcripts
    params:
        outdir = OUTDIR / "TALON" / 'filtered',
        annotation_name = existing_annotation.name.split('.')[0],
    output:
        GTF = OUTDIR / "TALON" / 'filtered_talon.gtf'
    singularity:
        "docker://biocontainers/talon:v5.0_cv1"
    shell:
        '''
        talon_create_GTF \
            --db {input.db} \
            --whitelist {input.filter} \
            -a {params.annotation_name} \
            --build "SIRV" \
            --o {params.outdir}
        '''

#FLAIR
# rule flair_bam_to_bed12:
#     input:
#         bam = rules.sam_to_bam.output.bam
#     params:
#         outdir = lambda wildcards: OUTDIR / "FLAIR" / "BED12" / wildcards.sample
#     output:
#         bed12 = OUTDIR / "FLAIR" / "BED12" / "{sample}.bed"
#     threads: 1
#     conda:
#         "envs/flair_conda_env.yaml"
#     shell:
#         '''
#         scripts/bam2Bed12.py --input_bam {input.bam} > {output.bed12}
#         '''


rule flair_align:
    '''
    Aligns samples against reference genome and smooths gaps in
    the alignment.
    '''
    input:
        genome = genome_fasta,
        fq = READSDIR / "{sample}.fastq"
    params:
        outdir = lambda wildcards: OUTDIR / "FLAIR" / "BED12" / wildcards.sample
    output:
        bed = OUTDIR / "FLAIR" / "BED12" / "{sample}.bed"
    threads: 10
    singularity:
        'docker://quay.io/biocontainers/flair:1.5--hdfd78af_4'
    shell:
        '''
        flair.py align \
            --genome {input.genome}\
            --reads {input.fq}\
            --threads {threads}\
            --nvrna \
            --version1.3 \
            --output {params.outdir}
        '''

rule flair_correct:
    '''
    Corrects misaligned splice sites using genome annotations
    and/or short-read splice junctions.
    '''
    input:
        genome = genome_fasta,
        annotation= existing_annotation,
        bed = rules.flair_align.output.bed
    params:
        outdir = lambda wildcards: OUTDIR / "FLAIR" / "corrected" / wildcards.sample / wildcards.sample,
        window = config["flair_correct_window"]
    output:
        bed_corrected = OUTDIR / "FLAIR" / "corrected" / "{sample}" / "{sample}_all_corrected.bed"
    threads: 10
    singularity:
        'docker://quay.io/biocontainers/flair:1.5--hdfd78af_4'
    shell:
        '''
        flair.py correct \
            --genome {input.genome} \
            --query {input.bed} \
            --gtf {input.annotation} \
            --nvrna \
            --threads {threads} \
            --window {params.window} \
            --output {params.outdir}
        '''

rule flair_concatenate:
    '''
    Combines BED12 output into one file.
    '''
    input:
        bed_corrected = expand(OUTDIR / "FLAIR" / "corrected" / "{sample}" / "{sample}_all_corrected.bed", sample=[".".join(read.name.split('.')[:-1]) for read in READS])
    output:
        bed_concatenated = OUTDIR / "FLAIR" / "concatenated_all_corrected.bed"
    threads: 10
    singularity:
        'docker://quay.io/biocontainers/flair:1.5--hdfd78af_4'
    shell:
        '''
        cat {input.bed_corrected} >> {output.bed_concatenated}
        '''

rule flair_collapse:
    '''
    Defines high-confidence isoforms from corrected reads.
    '''
    input:
        bed_concatenated = rules.flair_concatenate.output.bed_concatenated,
        genome = genome_fasta,
        annotation = existing_annotation,
    params:
        reads = READS,
        temp_dir = OUTDIR / "FLAIR" / "COLLAPSE" / "collapse_logs",
        outdir = OUTDIR / "FLAIR" / "COLLAPSE" / "flair.collapse",
        quality = config["flair_collapse_quality"]
    output:
        fa = OUTDIR / "FLAIR" / "COLLAPSE" / "flair.collapse.isoforms.fa"
    threads: 10
    singularity:
        'docker://quay.io/biocontainers/flair:1.5--hdfd78af_4'
    shell:
        '''
        flair.py collapse \
            --genome {input.genome} \
            --gtf {input.annotation} \
            --reads {params.reads} \
            --query {input.bed_concatenated} \
            --temp_dir {params.temp_dir} \
            --generate_map \
            --threads {threads} \
            --quality {params.quality} \
            --output {params.outdir}
        '''

rule flair_config:
    '''
    Creates read manifest.
    '''
    input:
        reads = READS
    params:
        datasetnames= [".".join(read.name.split('.')[:-1]) for read in READS]
    output:
        config = OUTDIR / "FLAIR" / "manifest.tsv"
    threads: 1
    run:
        for read, name in zip(input.reads, params.datasetnames):
            with open(output.config, 'a+') as config:
                config.write("%s\tcondition\tbatch\t%s\n" % (name, read))

rule flair_quantify:
    '''
    Quantify FLAIR isoform usage across samples using minimap2.
    '''
    input:
        manifest = rules.flair_config.output.config,
        coll_fasta = rules.flair_collapse.output.fa
    params:
        quality = config["flair_abundance_quality"]
    output:
        abundance = OUTDIR / "FLAIR" / "quantify" / "flair_counts_matrix.tsv"
    threads: 10
    singularity:
        'docker://quay.io/biocontainers/flair:1.5--hdfd78af_4'
    shell:
        '''
        flair.py quantify \
            --reads_manifest {input.manifest} \
            --isoforms {input.coll_fasta} \
            --threads {threads} \
            --tpm \
            --quality {params.quality} \
            --output {output.abundance}
        '''


#OXFORD

ContextFilter = """AlnContext: { Ref: "%s", LeftShift: -%d, RightShift: %d, RegexEnd: "[Aa]{%d,}", Stranded: True, Invert: True, Tsv: "alignments/internal_priming_fail.tsv"} """ % (genome_fasta,
config["oxford_poly_context"], config["oxford_poly_context"], config["oxford_max_poly_run"])

rule oxford_filter:
    input:
        genome = genome_fasta,
        fq = READSDIR / "{sample}.fastq",
        index = rules.build_minimap_index.output.index
    params:
        min_mq = config["oxford_minimum_mapping_quality"],
        flt = lambda x: ContextFilter,
        opts = config["minimap2_opts"]
    output:
        bam_filtered = OUTDIR / "OXFORD" / "filtered"/ "{sample}.bam"
    threads: 10
    conda:
        "envs/oxford_filter.yaml"
    shell:
        '''
        minimap2 -t {threads} -ax splice {params.opts} {input.index} {input.fq}\
        | samtools view -q {params.min_mq} -F 2304 -Sb -\
        | seqkit bam -j {threads} -x -T '{params.flt}' -\
        | samtools sort -@ {threads} -o {output.bam_filtered} -;
        samtools index {output.bam_filtered}
        '''

rule oxford_run_stringtie:
    input:
        bam_filtered = rules.oxford_filter.output.bam_filtered,
        annotation = existing_annotation,
    params:
        opts = config["oxford_stringtie_opts"],
    output:
        gff = OUTDIR / "OXFORD" / "stringtie_output" / "{sample}.gff",
    singularity:
        "docker://quay.io/biocontainers/stringtie:2.2.1--hecb563c_2"
    threads: 10
    shell:
        '''
        stringtie --rf -G {input.annotation} -L -p {threads} {params.opts} -o {output.gff} {input.bam_filtered}
        '''

rule oxford_merge_stringtie:
    input:
        gff_files = expand(OUTDIR / "OXFORD" / "stringtie_output" / "{sample}.gff", sample=[".".join(read.name.split('.')[:-1]) for read in READS]),
        annotation = existing_annotation
    params:
        opts = config["oxford_merge_opts"],
    output:
        merged_gff = OUTDIR / "OXFORD" / "stringtie_output" / "oxford_merged.gtf"
    singularity:
        "docker://quay.io/biocontainers/stringtie:2.2.1--hecb563c_2"
    threads: 10
    shell:
        '''
        stringtie \
            --merge {input.gff_files} \
            -G {input.annotation} \
            {params.opts} \
            -o {output.merged_gff}
        '''

rule oxford_abundance:
    input:
        bam = rules.oxford_filter.output.bam_filtered,
        merged_gtf = rules.oxford_merge_stringtie.output.merged_gff
    params:
        opts = config["oxford_count_opts"],
    output:
        count_gtf = OUTDIR / "OXFORD" / "abundance" / "{sample}.gtf"
    singularity:
        "docker://quay.io/biocontainers/stringtie:2.2.1--hecb563c_2"
    threads: 10
    shell:
        '''
        stringtie \
            -G {input.merged_gtf} \
            -e \
            {params.opts} \
            -o {output.count_gtf} \
            {input.bam}
        '''

rule oxford_config:
    input:
        count_gtf = expand(OUTDIR / "OXFORD" / "abundance" / "{sample}.gtf", sample=[".".join(read.name.split('.')[:-1]) for read in READS]),
    params:
        datasetnames= [".".join(read.name.split('.')[:-1]) for read in READS]
    output:
        config = OUTDIR / "OXFORD" / "abundance" / "oxford_config.tab"
    threads: 1
    run:
        for read, name in zip(input.count_gtf, params.datasetnames):
            with open(output.config, 'a+') as config:
                config.write("%s\t%s\n" % (name, read))

rule oxford_calc_abundance:
    input:
        config_file = rules.oxford_config.output.config
    params:
        avg_len = config["oxford_avg_len"],
    output:
        transcripts = OUTDIR / "OXFORD" / "abundance" / "transcript_count_matrix.csv",
        genes = OUTDIR / "OXFORD" / "abundance" / "gene_count_matrix.csv",
    threads: 10
    singularity:
        "docker://quay.io/biocontainers/stringtie:2.2.1--hecb563c_2"
    shell:
        '''
        prepDE.py \
            -i {input.config_file} \
            -l {params.avg_len} \
            -g {output.genes} \
            -t {output.transcripts}
        '''

