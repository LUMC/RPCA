
from pathlib import Path

# Read configuration
configfile: "config/config.yaml"

SOURCEDIR = Path(config["sourcedir"])
OUTDIR = Path(config["results"])
READSDIR = Path(config["readsdir"])
genome_fasta = Path(config["genome_fasta"])
existing_annotation = Path(config["existing_annotation"])


READS = [filepath for filepath in Path(READSDIR).glob('**/*')]

rule all:
    input:
        # flair
        # abundance = OUTDIR / "TALON" / 'filtered_talon_abundance_filtered.tsv',
        # GTF = OUTDIR / "TALON" / 'filtered_talon.gtf',
        # talon
        abundance = OUTDIR / "TALON" / 'filtered_talon_abundance_filtered.tsv',
        GTF = OUTDIR / "TALON" / 'filtered_talon.gtf',
        # oxford
        # transcripts = OUTDIR / "OXFORD" / "abundance" / "transcript_count_matrix.csv",
        # alignment stats
        # stats = OUTDIR / "alignments" / "stats" / "{sample}.stats"
        # tracking file
        #tracking = OUTDIR / "gffcompare" / "gffcmp.tracking"


rule minimap2_align:
    '''
    Align reads to reference genome.
    '''
    input:
        genome = genome_fasta,
        fq = READSDIR / "{sample}.fastq"
    params:
        outdir = OUTDIR,
        opts = config["minimap2_opts"]
    output:
        sam_files = OUTDIR / "alignments" /"{sample}.sam"
    threads: 10
    singularity:
        'docker://quay.io/biocontainers/minimap2:2.17--h5bf99c6_4'
    shell:
        '''
        minimap2 \
            -t {threads} \
            -ax splice \
            {params.opts} \
            {input.genome} \
            {input.fq} > {output.sam_files}
        '''

rule build_minimap_index:
    input:
        genome = genome_fasta
    output:
        index = OUTDIR / "alignments" / "genome_index.mmi"
    params:
        opts = config["minimap_index_opts"]
    singularity:
        'docker://quay.io/biocontainers/minimap2:2.17--h5bf99c6_4'
    threads: 10
    shell:
        """
        minimap2 -t {threads} {params.opts} -I 1000G -d {output.index} {input.genome}
        """

rule sam_to_bam:
    '''
    Converts SAM to BAM. 
    '''
    input:
        sam = rules.minimap2_align.output
    params:
        outdir = lambda wildcards: OUTDIR / "alignments" / "bam" / wildcards.sample
    output:
        bam = OUTDIR / "alignments" / "bam" / "{sample}_sorted.bam"
    threads: 10
    singularity:
        "docker://quay.io/biocontainers/samtools:1.14--hb421002_0"
    shell:
        '''
        samtools view -Sb {input.sam} | samtools sort -@ {threads} -o {output.bam}
        samtools index {output.bam}
        '''

# TALON
rule get_SJs_from_gtf:
    '''
    Extracts splice junctions from annotation file for use with 
    TranscriptClean.
    '''
    input:
        annotation = existing_annotation,
        genome = genome_fasta
    params:
        outdir = OUTDIR
    output:
        splicejns = OUTDIR / "TALON" / "cleaned_alignments" / "spliceJns.txt"
    threads: 1
    singularity:
        "docker://biocontainers/transcriptclean:v2.0.2_cv1"
    shell:
        '''
        get_SJs_from_gtf \
            --f {input.annotation} \
            --g {input.genome} \
            --o {output.splicejns}
        '''

rule transcriptclean:
    '''
    Corrects artefactual noncanonical splice junctions.
    '''
    input:
        sam_files = rules.minimap2_align.output.sam_files,
        genome = genome_fasta,
        splicejns = rules.get_SJs_from_gtf.output.splicejns
    params:
        outdir = lambda wildcards: OUTDIR / "TALON" / "cleaned_alignments" / wildcards.sample / wildcards.sample
    output:
        clean_sam = OUTDIR / "TALON" /"cleaned_alignments" / "{sample}" / "{sample}_clean.sam"
    threads: 10
    singularity:
        "docker://biocontainers/transcriptclean:v2.0.2_cv1"
    shell:
        '''
        TranscriptClean \
            --sam {input.sam_files} \
            --genome {input.genome} \
            -t {threads} \
            --spliceJns {input.splicejns} \
            --outprefix {params.outdir}
        '''

rule talon_label_reads:
    '''
    Flag possible internal priming in reads.
    '''
    input:
        clean_sam = rules.transcriptclean.output.clean_sam,
        # sam_files = rules.minimap2_align.output.sam_files,
        genome = genome_fasta
    params:
        outdir = lambda wildcards: OUTDIR / "TALON" / "labeled" / wildcards.sample
    output:
        labeled_sam = OUTDIR / "TALON" / "labeled" / "{sample}_labeled.sam",
    threads: 10
    singularity:
        "docker://biocontainers/talon:v5.0_cv1"
    shell:
        '''
        mkdir -p labeled
        talon_label_reads \
            --f {input.clean_sam}\
            --g {input.genome} \
            --t {threads} \
            --ar 20 \
            --o {params.outdir}
        '''

rule create_talon_config:
    '''
    Create configuration file containing labeled reads locations.
    '''
    input:
        labels = expand(OUTDIR / "TALON" / "labeled" / "{sample}_labeled.sam",sample=[".".join(read.name.split('.')[:-1]) for read in READS]),
    params:
        datasetnames= [".".join(read.name.split('.')[:-1]) for read in READS]
    output:
        config = OUTDIR / "TALON" / "config.csv",
    threads: 1
    run:
        for label, name in zip(input.labels, params.datasetnames):
            with open(output.config, 'a+') as config:
                config.write("%s,%s,ONT,%s\n" % (name, name, label))

rule talon_initialize_annotate_database:
    '''
    Initialize TALON database from annotation file.
    Annotate transcripts by comparing them to the database and update 
    database accordingly afterwards.
    '''
    input:
        annotation = existing_annotation,
        config = rules.create_talon_config.output.config
    params:
        outdir = OUTDIR / "TALON" / "talon",
        dbloc = OUTDIR / "TALON" / "talon.db",
        annotation_name = existing_annotation.name.split('.')[0],
        genome_name = genome_fasta.name.split('.')[0],
        end5 = config["talon_5p"],
        end3 = config["talon_3p"],
    output:
        database = OUTDIR / "TALON" / "talon.db"
    threads: 10
    singularity:
        "docker://biocontainers/talon:v5.0_cv1"
    shell:
        '''
        
        talon_initialize_database \
            --f {input.annotation} \
            --a {params.annotation_name} \
            --g {params.genome_name} \
            --5p {params.end5} \
            --3p {params.end3} \
            --o {params.outdir}
        echo "completed init"
        
        talon \
            --f {input.config} \
            --db {params.dbloc} \
            --build {params.genome_name} \
            --o {params.outdir} \
            -t {threads}
        echo "completed annot"
        '''

rule talon_filter_transcripts:
    '''
    Create whitelist of transcripts that passed filter.
    '''
    input:
        db = rules.talon_initialize_annotate_database.output.database
    params:
        outdir = OUTDIR,
        datasetnames = ",".join([".".join(read.name.split('.')[:-1]) for read in READS]),
        mincount = config["talon_mincounts"],
        mindatasets = config["talon_mindatasets"],
        annotation_name = existing_annotation.name.split('.')[0],
    output:
        filtered_transcripts = OUTDIR / "TALON" / "filtered_transcripts.csv"
    threads: 1
    singularity:
        "docker://biocontainers/talon:v5.0_cv1"
    shell:
        '''
        talon_filter_transcripts \
            --db {input.db} \
            --datasets {params.datasetnames} \
            -a {params.annotation_name} \
            --maxFracA 0.5 \
            --minCount {params.mincount} \
            --minDatasets {params.mindatasets} \
            --o {output.filtered_transcripts}
        '''

rule talon_abundance:
    '''
    Calculate abundance for transcripts in whitelist.
    '''
    input:
        db = rules.talon_initialize_annotate_database.output.database,
        filter = rules.talon_filter_transcripts.output.filtered_transcripts
    params:
        outdir = OUTDIR / "TALON" / 'filtered',
        annotation_name = existing_annotation.name.split('.')[0],
        genome_name= genome_fasta.name.split('.')[0],
    output:
        abundance = OUTDIR / "TALON" / 'filtered_talon_abundance_filtered.tsv'
    singularity:
        "docker://biocontainers/talon:v5.0_cv1"
    shell:
        '''
        talon_abundance \
            --db {input.db} \
            --whitelist {input.filter} \
            -a {params.annotation_name} \
            --build {params.genome_name} \
            --o {params.outdir}
        '''

rule talon_create_GTF:
    '''
    Extract transcripts from whitelist as GTF.
    '''
    input:
        db = rules.talon_initialize_annotate_database.output.database,
        filter= rules.talon_filter_transcripts.output.filtered_transcripts
    params:
        outdir = OUTDIR / "TALON" / 'filtered',
        annotation_name = existing_annotation.name.split('.')[0],
        genome_name= genome_fasta.name.split('.')[0],
    output:
        GTF = OUTDIR / "TALON" / 'filtered_talon.gtf'
    singularity:
        "docker://biocontainers/talon:v5.0_cv1"
    shell:
        '''
        talon_create_GTF \
            --db {input.db} \
            --whitelist {input.filter} \
            -a {params.annotation_name} \
            --build {params.genome_name} \
            --o {params.outdir}
        '''

#FLAIR
rule flair_bam_to_bed12:
    input:
        bam = rules.sam_to_bam.output.bam
    params:
        outdir = lambda wildcards: OUTDIR / "FLAIR" / "bed12" / wildcards.sample
    output:
        bed12 = OUTDIR / "FLAIR" / "bed12" / "{sample}.bed"
    threads: 1
    conda:
        "/exports/sascstudent/dbayraktar/miniconda3/envs/flair"
    shell:
        '''
        scripts/bam2Bed12.py --input_bam {input.bam} > {output.bed12}
        '''


rule flair_align:
    '''
    Aligns samples against reference genome and smooths gaps in
    the alignment.
    '''
    input:
        genome = genome_fasta,
        fq = READSDIR / "{sample}.fastq"
    params:
        outdir = lambda wildcards: OUTDIR / "FLAIR" / "bed12" / wildcards.sample
    output:
        bed = OUTDIR / "FLAIR" / "bed12" / "{sample}.bed"
    threads: 10
    singularity:
        'docker://quay.io/biocontainers/flair:1.5--hdfd78af_4'
    shell:
        '''
        flair.py align \
            --genome {input.genome}\
            --reads {input.fq}\
            --threads {threads}\
            --nvrna \
            --version1.3 \
            --output {params.outdir}
        '''

rule flair_correct:
    '''
    Corrects misaligned splice sites using genome annotations
    and/or short-read splice junctions.
    '''
    input:
        genome = genome_fasta,
        annotation= existing_annotation,
        # bed = rules.flair_align.output.bed
        bed = rules.flair_bam_to_bed12.output.bed12
    params:
        outdir = lambda wildcards: OUTDIR / "FLAIR" / "corrected" / wildcards.sample / wildcards.sample,
        window = config["flair_correct_window"]
    output:
        bed_corrected = OUTDIR / "FLAIR" / "corrected" / "{sample}" / "{sample}_all_corrected.bed"
    threads: 10
    singularity:
        'docker://quay.io/biocontainers/flair:1.5--hdfd78af_4'
    shell:
        '''
        flair.py correct \
            --genome {input.genome} \
            --query {input.bed} \
            --gtf {input.annotation} \
            --nvrna \
            --threads {threads} \
            --window {params.window} \
            --output {params.outdir}
        '''

rule flair_concatenate:
    '''
    Combines BED12 output into one file.
    '''
    input:
        bed_corrected = expand(OUTDIR / "FLAIR" / "corrected" / "{sample}" / "{sample}_all_corrected.bed", sample=[".".join(read.name.split('.')[:-1]) for read in READS])
    output:
        bed_concatenated = OUTDIR / "FLAIR" / "concatenated_all_corrected.bed"
    threads: 10
    singularity:
        'docker://quay.io/biocontainers/flair:1.5--hdfd78af_4'
    shell:
        '''
        cat {input.bed_corrected} >> {output.bed_concatenated}
        '''

rule flair_collapse:
    '''
    Defines high-confidence isoforms from corrected reads.
    '''
    input:
        bed_concatenated = rules.flair_concatenate.output.bed_concatenated,
        genome = genome_fasta,
        annotation = existing_annotation,
    params:
        reads = READS,
        temp_dir = OUTDIR / "FLAIR" / "collapse" / "collapse_logs",
        outdir = OUTDIR / "FLAIR" / "collapse" / "flair.collapse",
        window = config["flair_collapse_window"],
        quality = config["flair_collapse_quality"],
        support = config["flair_collapse_support"],
        max_ends = config["flair_collapse_max_ends"],
        opts = config["flair_collapse_optional"]
    output:
        fa = OUTDIR / "FLAIR" / "collapse" / "flair.collapse.isoforms.fa",
        gtf = OUTDIR / "FLAIR" / "collapse" / "flair.collapse.isoforms.gtf"
    threads: 10
    singularity:
        'docker://quay.io/biocontainers/flair:1.5--hdfd78af_4'
    shell:
        '''
        flair.py collapse \
            --genome {input.genome} \
            --gtf {input.annotation} \
            --reads {params.reads} \
            --query {input.bed_concatenated} \
            --temp_dir {params.temp_dir} \
            --generate_map \
            --threads {threads} \
            --window {params.window} \
            --quality {params.quality} \
            --support {params.support} \
            --max_ends {params.max_ends} \
            {params.opts} \
            --output {params.outdir}
        '''

rule flair_config:
    '''
    Creates read manifest.
    '''
    input:
        reads = READS
    params:
        datasetnames= [".".join(read.name.split('.')[:-1]) for read in READS]
    output:
        config = OUTDIR / "FLAIR" / "manifest.tsv"
    threads: 1
    run:
        for read, name in zip(input.reads, params.datasetnames):
            with open(output.config, 'a+') as config:
                config.write("%s\tcondition\tbatch\t%s\n" % (name, read))

rule flair_quantify:
    '''
    Quantify FLAIR isoform usage across samples using minimap2.
    '''
    input:
        manifest = rules.flair_config.output.config,
        coll_fasta = rules.flair_collapse.output.fa
    params:
        quality = config["flair_abundance_quality"]
    output:
        abundance = OUTDIR / "FLAIR" / "quantify" / "flair_counts_matrix.tsv"
    threads: 10
    singularity:
        'docker://quay.io/biocontainers/flair:1.5--hdfd78af_4'
    shell:
        '''
        flair.py quantify \
            --reads_manifest {input.manifest} \
            --isoforms {input.coll_fasta} \
            --threads {threads} \
            --tpm \
            --quality {params.quality} \
            --output {output.abundance}
        '''


#OXFORD

ContextFilter = """AlnContext: { Ref: "%s", LeftShift: -%d, RightShift: %d, RegexEnd: "[Aa]{%d,}", Stranded: True, Invert: True, Tsv: "alignments/internal_priming_fail.tsv"} """ % (genome_fasta,
config["oxford_poly_context"], config["oxford_poly_context"], config["oxford_max_poly_run"])

rule oxford_filter:
    '''
    Align samples filter for quality
    and internal priming, convert to BAM.
    '''
    input:
        genome = genome_fasta,
        fq = READSDIR / "{sample}.fastq",
        index = rules.build_minimap_index.output.index
    params:
        min_mq = config["oxford_minimum_mapping_quality"],
        flt = lambda x: ContextFilter,
        opts = config["minimap2_opts"]
    output:
        bam_filtered = OUTDIR / "OXFORD" / "filtered"/ "{sample}.bam"
    threads: 10
    conda:
        "envs/oxford_filter.yaml"
    shell:
        '''
        minimap2 -t {threads} -ax splice {params.opts} {input.index} {input.fq}\
        | samtools view -q {params.min_mq} -F 2304 -Sb -\
        | seqkit bam -j {threads} -x -T '{params.flt}' -\
        | samtools sort -@ {threads} -o {output.bam_filtered} -;
        samtools index {output.bam_filtered}
        '''

rule oxford_run_stringtie:
    '''
    Run stringtie for annotation per sample.
    '''
    input:
        bam_filtered = rules.oxford_filter.output.bam_filtered,
        annotation = existing_annotation,
    params:
        opts = config["oxford_stringtie_opts"],
    output:
        gff = OUTDIR / "OXFORD" / "stringtie_output" / "{sample}.gff",
    singularity:
        "docker://quay.io/biocontainers/stringtie:2.2.1--hecb563c_2"
    threads: 10
    shell:
        '''
        stringtie --rf -G {input.annotation} -L -p {threads} {params.opts} -o {output.gff} {input.bam_filtered}
        '''

rule oxford_merge_stringtie:
    '''
    Merge sample annotations to single file.
    '''
    input:
        gff_files = expand(OUTDIR / "OXFORD" / "stringtie_output" / "{sample}.gff", sample=[".".join(read.name.split('.')[:-1]) for read in READS]),
        annotation = existing_annotation
    params:
        opts = config["oxford_merge_opts"],
    output:
        merged_gff = OUTDIR / "OXFORD" / "stringtie_output" / "oxford_merged.gtf"
    singularity:
        "docker://quay.io/biocontainers/stringtie:2.2.1--hecb563c_2"
    threads: 10
    shell:
        '''
        stringtie \
            --merge {input.gff_files} \
            -G {input.annotation} \
            {params.opts} \
            -o {output.merged_gff}
        '''

rule oxford_abundance:
    '''
    Run stringtie in expression estimation mode for all samples.
    '''
    input:
        bam = rules.oxford_filter.output.bam_filtered,
        merged_gtf = rules.oxford_merge_stringtie.output.merged_gff
    params:
        opts = config["oxford_count_opts"],
    output:
        count_gtf = OUTDIR / "OXFORD" / "abundance" / "{sample}.gtf"
    singularity:
        "docker://quay.io/biocontainers/stringtie:2.2.1--hecb563c_2"
    threads: 10
    shell:
        '''
        stringtie \
            -G {input.merged_gtf} \
            -e \
            {params.opts} \
            -o {output.count_gtf} \
            {input.bam}
        '''

rule oxford_config:
    '''
    Create configuration file containing location of abundance
    GTF files.
    '''
    input:
        count_gtf = expand(OUTDIR / "OXFORD" / "abundance" / "{sample}.gtf", sample=[".".join(read.name.split('.')[:-1]) for read in READS]),
    params:
        datasetnames= [".".join(read.name.split('.')[:-1]) for read in READS]
    output:
        config = OUTDIR / "OXFORD" / "abundance" / "oxford_config.tab"
    threads: 1
    run:
        for read, name in zip(input.count_gtf, params.datasetnames):
            with open(output.config, 'a+') as config:
                config.write("%s\t%s\n" % (name, read))

rule oxford_calc_abundance:
    '''
    Calculate abundance.
    '''
    input:
        config_file = rules.oxford_config.output.config
    params:
        avg_len = config["oxford_avg_len"],
    output:
        transcripts = OUTDIR / "OXFORD" / "abundance" / "transcript_count_matrix.csv",
        genes = OUTDIR / "OXFORD" / "abundance" / "gene_count_matrix.csv",
    threads: 10
    singularity:
        "docker://quay.io/biocontainers/stringtie:2.2.1--hecb563c_2"
    shell:
        '''
        prepDE.py \
            -i {input.config_file} \
            -l {params.avg_len} \
            -g {output.genes} \
            -t {output.transcripts}
        '''

#analyses

rule align_stats:
    input:
        bam = rules.sam_to_bam.output.bam,
        reference = genome_fasta,
    params:
        outdir = lambda wildcards: OUTDIR / "alignments" / "stats" / wildcards.sample
    output:
        stats = OUTDIR  / "statistics" / "{sample}.stats"
    threads: 10
    singularity:
        "docker://quay.io/biocontainers/samtools:1.14--hb421002_0"
    shell:
        '''
        samtools stats \
            --reference {input.reference} \
            --threads {threads} \
            {input.bam} | grep ^SN | cut -f 2- > {output.stats}
        '''

rule annotation_count:
    input:
        bams = expand(OUTDIR / "alignments" / "bam" / "{sample}_sorted.bam",sample=[".".join(read.name.split('.')[:-1]) for read in READS]),
        annotation = existing_annotation,
    output:
        counts = OUTDIR / "statistics" / "annotation_count.txt"
    threads: 10
    singularity:
        "docker://"
    shell:
        '''
        subread featurecounts \
            {input.bams} \
            -T {threads} \
            -a {input.annotation} \
            -L \
            -M \
            -S 1 \
            -o {output.counts}
        '''

rule combine:
    input:
        oxford_count = rules.oxford_calc_abundance.output.transcripts,
        oxford_transcript = rules.oxford_merge_stringtie.output.merged_gff,
        talon_count = rules.talon_abundance.output.abundance,
        talon_transcripts = rules.talon_create_GTF.output.GTF,
        flair_count = rules.flair_quantify.output.abundance,
        flair_transcripts = rules.flair_collapse.output.gtf,
    output:
        oxford_combined = OUTDIR / "combined" / "oxford_counts.gtf",
        flair_combined = OUTDIR / "combined" / "flair_counts.gtf",
        talon_combined = OUTDIR / "combined" / "talon_counts.gtf",
    threads: 1
    script:
        '''
        combine.py 
        '''

rule gffcompare:
    input:
        oxford = rules.combine.output.oxford_combined,
        flair = rules.combine.output.flair_combined,
        talon = rules.combine.output.talon_combined,
        annotation = existing_annotation
    output:
        tracking = OUTDIR / "gffcompare" / "gffcmp.tracking"
    threads: 1
    singularity:
        "docker://quay.io/biocontainers/gffcompare:0.11.2--h7d875b9_2"
    shell:
        '''
        gffcompare \
            -r {input.annotation} \
            -e 100 \
            -d 100 \
            -V \
            --no-merge \
            {input.oxford} {input.flair} {input.talon}
        '''

rule venn_diagram:
    input:
        tracking = rules.gffcompare.output.tracking
    output:
        all = OUTDIR / "graphs" / "diagram_all.png",
        novel = OUTDIR / "graphs" / "diagram_novel.png",
        known = OUTDIR / "graphs" / "diagram_known.png"
    threads: 1
    script:
        '''
        venndiagrams.py
        '''

rule plot_stats:
    input:
        alignstats = rules.align_stats.output.stats,
        annocount = rules.annotation_count.output.counts,
        flair_count = rules.flair_quantify.output.abundance,
        talon_count = rules.talon_abundance.output.abundance,
        oxford_count = rules.oxford_abundance.output.count_gtf,
    output:
        readalign = OUTDIR / "graphs" / "read_alignments.png",
        mismatch = OUTDIR / "graphs" / "mismatches.png",
        readlen = OUTDIR / "graphs" / "readlengths.png",
        anno_count = OUTDIR / "graphs" / "anno_count_sample.png",
        anno_count_all = OUTDIR / "graphs" / "anno_count_total.png",
        count_comp = OUTDIR / "graphs" / "counts_per_pipeline.png"
    threads: 1
    conda:
        "envs/notebook.yaml"
    script:
        '''
        stats.py
        '''

